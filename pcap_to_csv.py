
#!/usr/bin/env python3
"""
QUIC pcap íŒŒì¼ ë¶„ì„ ë° í†µê³„ ìƒì„± ë„êµ¬
ê° QUIC flowì˜ ì „ì²´ í†µê³„ì™€ ì²« 5, 10, 15, 20ê°œ íŒ¨í‚·ì— ëŒ€í•œ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
tsharkë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸ ì²˜ë¦¬.
"""

import os
import sys
import platform
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
from collections import defaultdict
from tqdm import tqdm
import time
import subprocess
import numpy as np

# Configuration Import
import config

# --- Use config constants ---
PCAP_ROOT_DIR = config.PCAP_ROOT_DIR
OUTPUT_DIR = config.OUTPUT_DIR
PACKET_WINDOWS = config.PACKET_WINDOWS
TSHARK_CMD = config.TSHARK_CMD
OUTPUT_FOLDERS = config.OUTPUT_FOLDERS


def find_all_pcap_files(root_dir: Path) -> List[Path]:
    """
    ëª¨ë“  í´ë”(ì¬ê·€ì )ì—ì„œ ëª¨ë“  pcap íŒŒì¼ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    pcap_files = []
    
    if not root_dir.exists():
        print(f"ê²½ê³ : {root_dir} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return pcap_files
    
    # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  pcap íŒŒì¼ ì°¾ê¸°
    pcap_files = sorted(root_dir.rglob("*.pcap"))
    
    # íŒŒì¼ë§Œ í•„í„°ë§ (ë””ë ‰í† ë¦¬ ì œì™¸)
    pcap_files = [f for f in pcap_files if f.is_file()]
    
    return pcap_files


def format_file_size(size_bytes: int) -> str:
    """íŒŒì¼ í¬ê¸°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} PB"


def extract_quic_flows_tshark(pcap_file: Path) -> Dict[str, List[Dict[str, Any]]]:
    """
    tsharkë¥¼ ì‚¬ìš©í•˜ì—¬ pcap íŒŒì¼ì—ì„œ QUIC flowë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    flows: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    
    file_size = pcap_file.stat().st_size
    file_size_str = format_file_size(file_size)
    
    print(f"  ğŸ“– íŒŒì¼ ì½ê¸°: {pcap_file.name} ({file_size_str})")
    
    try:
        # tshark ëª…ë ¹ì–´ - QUIC í”„ë¡œí† ì½œ íŠ¹ì • í•„ë“œ ì¶”ê°€
        cmd = [
            TSHARK_CMD,
            '-r', str(pcap_file),
            '-Y', 'quic || udp.port == 443',
            '-T', 'fields',
            '-E', 'separator=|',
            '-E', 'quote=d',
            '-e', 'ip.src',
            '-e', 'ip.dst',
            '-e', 'ipv6.src',
            '-e', 'ipv6.dst',
            '-e', 'udp.srcport',
            '-e', 'udp.dstport',
            '-e', 'frame.len',
            '-e', 'frame.time_epoch',
            '-e', 'quic.long.packet_type',
            '-e', 'quic.packet_length',
            '-e', 'quic.version',
            '-e', 'quic.dcid',
            '-e', 'quic.scid'
        ]
        
        # tshark ì‹¤í–‰
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        
        # ì¶œë ¥ íŒŒì‹±
        lines = result.stdout.strip().split('\n')
        
        with tqdm(total=len(lines), desc="  íŒ¨í‚· ì²˜ë¦¬", unit="pkt", leave=False) as pbar:
            for line in lines:
                if not line.strip():
                    continue
                
                # íŒŒì´í”„ë¡œ êµ¬ë¶„ëœ í•„ë“œ íŒŒì‹± (ë”°ì˜´í‘œ ì œê±°)
                fields = [f.strip('"').strip() for f in line.split('|')]
                
                if len(fields) < 8:
                    pbar.update(1)
                    continue
                
                # í•„ë“œ ì¶”ì¶œ
                ip_src = fields[0]
                ip_dst = fields[1]
                ipv6_src = fields[2]
                ipv6_dst = fields[3]
                src_port = fields[4]
                dst_port = fields[5]
                frame_len = fields[6]
                time_epoch = fields[7]
                
                # QUIC í”„ë¡œí† ì½œ íŠ¹ì • í•„ë“œ (ì¸ë±ìŠ¤ 8ë¶€í„°)
                packet_type = fields[8] if len(fields) > 8 else ''
                quic_length = fields[9] if len(fields) > 9 else ''
                quic_version = fields[10] if len(fields) > 10 else ''
                dcid = fields[11] if len(fields) > 11 else ''
                scid = fields[12] if len(fields) > 12 else ''
                
                # IP ì£¼ì†Œ ê²°ì • (IPv4 ìš°ì„ , ì—†ìœ¼ë©´ IPv6)
                src_ip = ip_src if ip_src else ipv6_src
                dst_ip = ip_dst if ip_dst else ipv6_dst
                
                # í•„ìˆ˜ í•„ë“œ ì²´í¬
                if not src_ip or not dst_ip or not src_port or not dst_port:
                    pbar.update(1)
                    continue
                
                # Spin bit ì¶”ì¶œ (Short header íŒ¨í‚·ì—ë§Œ ì¡´ì¬, ì—¬ê¸°ì„œëŠ” íŒ¨í‚· íƒ€ì…ìœ¼ë¡œ ì¶”ì •)
                has_long_header = bool(packet_type)
                
                # íŒ¨í‚· ì •ë³´ ìƒì„±
                try:
                    packet_info = {
                        'src_ip': src_ip,
                        'dst_ip': dst_ip,
                        'src_port': src_port,
                        'dst_port': dst_port,
                        'size': int(frame_len) if frame_len else 0,
                        'timestamp': float(time_epoch) if time_epoch else 0.0,
                        'packet_type': packet_type.lower() if packet_type else '',
                        'is_long_header': has_long_header,
                        'is_short_header': not has_long_header,
                        'quic_length': int(quic_length) if quic_length else 0,
                        'quic_version': quic_version,
                        'dcid': dcid,
                        'scid': scid
                    }
                except (ValueError, TypeError):
                    pbar.update(1)
                    continue
                
                # Flow ID ìƒì„± (ì–‘ë°©í–¥ í†µí•©)
                flow_id_1 = f"{src_ip}:{src_port}->{dst_ip}:{dst_port}"
                flow_id_2 = f"{dst_ip}:{dst_port}->{src_ip}:{src_port}"
                flow_id = min(flow_id_1, flow_id_2)
                
                # ì²« ë²ˆì§¸ íŒ¨í‚·ì˜ ë°©í–¥ì„ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥
                if flow_id not in flows:
                    flows[flow_id] = {
                        'packets': [],
                        'server_ip': None,
                        'server_port': None,
                        'client_ip': None,
                        'client_port': None
                    }
                
                packet_info['direction'] = None
                flows[flow_id]['packets'].append(packet_info)
                
                pbar.update(1)
        
        print(f"  âœ“ {len(flows)}ê°œ flow, {len(lines)}ê°œ íŒ¨í‚· ë°œê²¬")
        
        # ê° flowì˜ ë°©í–¥ ì •ë³´ ì„¤ì • (ì²« íŒ¨í‚· ê¸°ì¤€)
        for flow_id, flow_data in flows.items():
            packets = flow_data['packets']
            if not packets:
                continue
            
            # ì²« íŒ¨í‚·ì˜ srcë¥¼ í´ë¼ì´ì–¸íŠ¸ë¡œ ê°€ì •
            first_packet = packets[0]
            flow_data['client_ip'] = first_packet['src_ip']
            flow_data['client_port'] = first_packet['src_port']
            flow_data['server_ip'] = first_packet['dst_ip']
            flow_data['server_port'] = first_packet['dst_port']
            
            # ëª¨ë“  íŒ¨í‚·ì— ë°©í–¥ ì •ë³´ ì¶”ê°€
            for packet in packets:
                if (packet['src_ip'] == flow_data['client_ip'] and 
                    packet['src_port'] == flow_data['client_port']):
                    packet['direction'] = 'outgoing'
                else:
                    packet['direction'] = 'incoming'
        
    except subprocess.CalledProcessError as e:
        print(f"  âŒ tshark ì‹¤í–‰ ì˜¤ë¥˜: {e.stderr}")
        raise
    except Exception as e:
        print(f"  âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise
    
    return flows


def calculate_entropy(values: List[Any]) -> float:
    """ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if not values:
        return 0.0
    
    # ê°’ì˜ ë¹ˆë„ ê³„ì‚°
    from collections import Counter
    counts = Counter(values)
    total = len(values)
    
    # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
    entropy = 0.0
    for count in counts.values():
        probability = count / total
        if probability > 0:
            entropy -= probability * np.log2(probability)
    
    return entropy


def calculate_comprehensive_statistics(packets: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    64ê°œ íŠ¹ì§•ì„ í¬í•¨í•œ í¬ê´„ì ì¸ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    [STRUCTURAL LEAKAGE PROTECTION] ì´ í•¨ìˆ˜ëŠ” ì „ë‹¬ë°›ì€ íŒ¨í‚· ë¦¬ìŠ¤íŠ¸ ì „ì²´ë§Œ ì²˜ë¦¬í•˜ë©°, 
    ì™¸ë¶€ì˜ ì „ì²´ Flow ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    """
    if not packets:
        return {}
    
    stats = {}
    
    # ë°©í–¥ë³„ë¡œ íŒ¨í‚· ë¶„ë¦¬
    all_packets = packets
    outgoing_packets = [p for p in packets if p.get('direction') == 'outgoing']
    incoming_packets = [p for p in packets if p.get('direction') == 'incoming']
    
    # === 1. íŒ¨í‚· ë° ë°”ì´íŠ¸ ìˆ˜ í†µê³„ ===
    stats['total_packets'] = len(all_packets)
    stats['outgoing_packets'] = len(outgoing_packets)
    stats['incoming_packets'] = len(incoming_packets)
    
    stats['total_bytes'] = sum(p.get('size', 0) for p in all_packets)
    stats['outgoing_bytes'] = sum(p.get('size', 0) for p in outgoing_packets)
    stats['incoming_bytes'] = sum(p.get('size', 0) for p in incoming_packets)
    
    # === 2. íŒ¨í‚· í¬ê¸° í†µê³„ (ì „ì²´, incoming, outgoing) ===
    def calc_size_stats(pkts, prefix):
        sizes = [p.get('size', 0) for p in pkts if p.get('size', 0) > 0]
        if not sizes:
            stats[f'{prefix}_mean'] = 0
            stats[f'{prefix}_min'] = 0
            stats[f'{prefix}_max'] = 0
            stats[f'{prefix}_std'] = 0
            stats[f'{prefix}_var'] = 0
            stats[f'{prefix}_cv'] = 0
            return
        
        stats[f'{prefix}_mean'] = np.mean(sizes)
        stats[f'{prefix}_min'] = np.min(sizes)
        stats[f'{prefix}_max'] = np.max(sizes)
        stats[f'{prefix}_std'] = np.std(sizes)
        stats[f'{prefix}_var'] = np.var(sizes)
        stats[f'{prefix}_cv'] = np.std(sizes) / np.mean(sizes) if np.mean(sizes) > 0 else 0
    
    calc_size_stats(all_packets, 'packet_size')
    calc_size_stats(outgoing_packets, 'packet_size_out')
    calc_size_stats(incoming_packets, 'packet_size_in')
    
    # === 3. IAT (Inter-Arrival Time) í†µê³„ ===
    def calc_iat_stats(pkts, prefix):
        timestamps = sorted([p.get('timestamp', 0) for p in pkts if p.get('timestamp', 0) > 0])
        if len(timestamps) < 2:
            stats[f'{prefix}_mean'] = 0
            stats[f'{prefix}_min'] = 0
            stats[f'{prefix}_max'] = 0
            stats[f'{prefix}_std'] = 0
            stats[f'{prefix}_var'] = 0
            return
        
        iats = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]
        stats[f'{prefix}_mean'] = np.mean(iats)
        stats[f'{prefix}_min'] = np.min(iats)
        stats[f'{prefix}_max'] = np.max(iats)
        stats[f'{prefix}_std'] = np.std(iats)
        stats[f'{prefix}_var'] = np.var(iats)
    
    calc_iat_stats(all_packets, 'iat')
    calc_iat_stats(outgoing_packets, 'iat_out')
    calc_iat_stats(incoming_packets, 'iat_in')
    
    # === 4. QUIC í”„ë¡œí† ì½œ íŠ¹ì • íŠ¹ì§• ===
    short_header_packets = [p for p in all_packets if p.get('is_short_header', False)]
    long_header_packets = [p for p in all_packets if p.get('is_long_header', False)]
    
    stats['short_header_count'] = len(short_header_packets)
    stats['long_header_count'] = len(long_header_packets)
    stats['short_header_ratio'] = len(short_header_packets) / len(all_packets) if all_packets else 0
    stats['long_header_ratio'] = len(long_header_packets) / len(all_packets) if all_packets else 0
    
    stats['spin_bit_count'] = 0
    stats['spin_bit_ratio'] = 0.0
    stats['no_spin_bit_ratio'] = 1.0
    
    for pkt in all_packets:
        ptype = pkt.get('packet_type', '').lower()
        
        if 'initial' in ptype or ptype == '0':
            stats['initial_packets'] = stats.get('initial_packets', 0) + 1
        elif 'handshake' in ptype or ptype == '2':
            stats['handshake_packets'] = stats.get('handshake_packets', 0) + 1
        elif '0-rtt' in ptype or 'zerortt' in ptype or ptype == '1':
            stats['zerortt_packets'] = stats.get('zerortt_packets', 0) + 1
        elif 'retry' in ptype or ptype == '3':
            stats['retry_packets'] = stats.get('retry_packets', 0) + 1
        elif pkt.get('is_short_header', False):
            stats['onertt_packets'] = stats.get('onertt_packets', 0) + 1
    
    for key in ['initial_packets', 'handshake_packets', 'zerortt_packets', 
                'onertt_packets', 'retry_packets']:
        if key not in stats:
            stats[key] = 0
    
    total = len(all_packets)
    if total > 0:
        stats['initial_ratio'] = stats['initial_packets'] / total
        stats['handshake_ratio'] = stats['handshake_packets'] / total
        stats['zerortt_ratio'] = stats['zerortt_packets'] / total
        stats['onertt_ratio'] = stats['onertt_packets'] / total
        stats['retry_ratio'] = stats['retry_packets'] / total
    else:
        stats['initial_ratio'] = 0
        stats['handshake_ratio'] = 0
        stats['zerortt_ratio'] = 0
        stats['onertt_ratio'] = 0
        stats['retry_ratio'] = 0
    
    # === 5. ì—”íŠ¸ë¡œí”¼ íŠ¹ì§• ===
    directions = [p.get('direction', 'unknown') for p in all_packets]
    stats['entropy_direction'] = calculate_entropy(directions)
    
    sizes_binned = [p.get('size', 0) // 10 for p in all_packets]
    stats['entropy_packet_size'] = calculate_entropy(sizes_binned)
    
    # === 6. Flow Duration ===
    timestamps = [p.get('timestamp', 0) for p in all_packets if p.get('timestamp', 0) > 0]
    if len(timestamps) >= 2:
        stats['duration'] = max(timestamps) - min(timestamps)
    else:
        stats['duration'] = 0
    
    return stats


def analyze_pcap_file(pcap_file: Path) -> Dict[str, int]:
    """pcap íŒŒì¼ì„ ë¶„ì„í•˜ê³  ê° ì¢…ë¥˜ë³„ë¡œ CSVë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    start_time = time.time()
    
    try:
        # QUIC flow ì¶”ì¶œ
        flows_data = extract_quic_flows_tshark(pcap_file)
        
        if not flows_data:
            print(f"  âš ï¸  flow ì—†ìŒ")
            return {'full': 0, 5: 0, 10: 0, 15: 0, 20: 0}
        
        filename_base = pcap_file.stem
        
        # ì „ì²´ flow í†µê³„ ê³„ì‚°
        print(f"  âš™ï¸  ì „ì²´ í†µê³„ ê³„ì‚° ì¤‘ (64ê°œ íŠ¹ì§•)...")
        full_results = []
        for flow_id, flow_data in tqdm(flows_data.items(), desc="  ì „ì²´ flow", leave=False):
            packets = flow_data['packets']
            
            flow_result = {
                'file': pcap_file.name,
                'flow_id': flow_id,
                'client_ip': flow_data.get('client_ip', ''),
                'client_port': flow_data.get('client_port', ''),
                'server_ip': flow_data.get('server_ip', ''),
                'server_port': flow_data.get('server_port', ''),
            }
            
            # 64ê°œ íŠ¹ì§• ê³„ì‚°
            full_stats = calculate_comprehensive_statistics(packets)
            flow_result.update(full_stats)
            
            full_results.append(flow_result)
        
        # ì „ì²´ í†µê³„ ì €ì¥
        if full_results:
            df = pd.DataFrame(full_results)
            output_path = OUTPUT_FOLDERS['full'] / f"{filename_base}.csv"
            df.to_csv(output_path, index=False)
            print(f"    âœ“ {output_path}")
        
        # ê° ìœˆë„ìš° í¬ê¸°ë³„ë¡œ í†µê³„ ê³„ì‚°
        window_counts = {str(w): 0 for w in PACKET_WINDOWS}
        
        # [LEAKAGE PROTECTION] Windowed ë°ì´í„°ì— í¬í•¨ë  ì•ˆì „í•œ ë©”íƒ€ë°ì´í„°ë§Œ ì •ì˜
        def get_safe_metadata(flow_id, flow_data, window):
            return {
                'file': pcap_file.name,
                'flow_id': flow_id,
                'window_size': window,
                'client_ip': flow_data.get('client_ip', ''),
                'client_port': flow_data.get('client_port', ''),
                'server_ip': flow_data.get('server_ip', ''),
                'server_port': flow_data.get('server_port', ''),
            }

        for window in PACKET_WINDOWS:
            print(f"  âš™ï¸  ì²« {window}ê°œ íŒ¨í‚· í†µê³„ ê³„ì‚° ì¤‘...")
            window_results = []
            
            for flow_id, flow_data in tqdm(flows_data.items(), desc=f"  ì²« {window}ê°œ", leave=False):
                packets = flow_data['packets']
                
                if len(packets) < window:
                    continue
                
                # [STRUCTURAL ISOLATION] íŒ¨í‚·ì„ ë¨¼ì € ìë¥¸ í›„ í†µê³„ í•¨ìˆ˜ì— ì „ë‹¬
                # ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ í†µê³„ í•¨ìˆ˜ëŠ” ë¯¸ë˜ì˜ íŒ¨í‚· ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
                windowed_packets = packets[:window]
                
                # ë©”íƒ€ë°ì´í„° ìƒì„± (ë¯¸ë˜ ì •ë³´ì¸ total_packets_in_flow ë“±ì´ í¬í•¨ë˜ì§€ ì•ŠìŒ)
                flow_result = get_safe_metadata(flow_id, flow_data, window)
                
                # ì˜ë ¤ì§„ íŒ¨í‚·ë“¤ë§Œì„ ëŒ€ìƒìœ¼ë¡œ í†µê³„ ê³„ì‚°
                window_stats = calculate_comprehensive_statistics(windowed_packets)
                flow_result.update(window_stats)
                
                window_results.append(flow_result)
                window_counts[str(window)] += 1
            
            # ìœˆë„ìš°ë³„ í†µê³„ ì €ì¥
            if window_results:
                df = pd.DataFrame(window_results)
                output_path = OUTPUT_FOLDERS[str(window)] / f"{filename_base}.csv"
                df.to_csv(output_path, index=False)
                print(f"    âœ“ {output_path}")
        
        elapsed = time.time() - start_time
        print(f"  âœ“ ì™„ë£Œ: {len(flows_data)} flows, {elapsed:.2f}ì´ˆ")
        
        return {'full': len(full_results), **window_counts}
        
    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return {'full': 0, 5: 0, 10: 0, 15: 0, 20: 0}


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("QUIC pcap íŒŒì¼ ë¶„ì„ ì‹œì‘ (64ê°œ íŠ¹ì§• ì¶”ì¶œ)")
    print(f"OS: {platform.system()}")
    print(f"tshark: {TSHARK_CMD}")
    print(f"PCAP ë£¨íŠ¸ ë””ë ‰í† ë¦¬: {PCAP_ROOT_DIR}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
    print("=" * 80)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
    config.setup_output_directories()
    print("âœ“ ì¶œë ¥ í´ë” êµ¬ì¡° ìƒì„± ì™„ë£Œ")
    
    # ëª¨ë“  í´ë”ì—ì„œ ëª¨ë“  pcap íŒŒì¼ ì°¾ê¸°
    pcap_files = find_all_pcap_files(PCAP_ROOT_DIR)
    
    if not pcap_files:
        print("âŒ ë¶„ì„í•  pcap íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ“ {len(pcap_files)}ê°œì˜ pcap íŒŒì¼ ë°œê²¬")
    for pcap_file in pcap_files:
        print(f"  - {pcap_file.relative_to(PCAP_ROOT_DIR)}")
    
    print("\n" + "=" * 80)
    print("ë¶„ì„ ì‹œì‘...")
    print("=" * 80 + "\n")
    
    # Dynamically initialize total_stats based on config
    total_stats = {key: 0 for key in config.DATASETS}
    
    for i, pcap_file in enumerate(pcap_files, 1):
        print(f"\n[{i}/{len(pcap_files)}] {pcap_file.name}")
        print("-" * 80)
        
        stats = analyze_pcap_file(pcap_file)
        
        # stats keys are strings ('full', '5', etc.)
        for key, value in stats.items():
            if key in total_stats:
                total_stats[key] += value
    
    print("\n" + "=" * 80)
    print("ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nì²˜ë¦¬ëœ Flow í†µê³„:")
    print(f"  - ì „ì²´ (full): {total_stats.get('full', 0)} flows")
    for window in PACKET_WINDOWS:
        print(f"  - ì²« {window}ê°œ íŒ¨í‚·: {total_stats.get(str(window), 0)} flows")
    
    print(f"\nê²°ê³¼ ì €ì¥ ìœ„ì¹˜:")
    for window_name, folder in OUTPUT_FOLDERS.items():
        csv_count = len(list(folder.glob("*.csv")))
        print(f"  - {folder}: {csv_count}ê°œ CSV íŒŒì¼")
    
    print("\nâœ“ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
